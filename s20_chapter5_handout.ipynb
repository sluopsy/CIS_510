{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDiYMkiYgRS"
      },
      "source": [
        "<center>\n",
        "<h1>Chapter 5</h1>\n",
        "</center>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcab0aNw7_f3"
      },
      "source": [
        "#First let's bring in puddles\n",
        "\n",
        "\n",
        "<img src='https://www.dropbox.com/s/zv84ew76m7ptyrh/puddles.jpeg?raw=1' height=100>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_GdU8ACeitW",
        "outputId": "bde93da1-5425-4730-eadc-b026ec891c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#flush the old directory\n",
        "!rm -r  'uo_puddles'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'uo_puddles': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPISZH5nclhU"
      },
      "source": [
        "my_github_name = 'uo-puddles'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuaxZXKXcrNL"
      },
      "source": [
        "clone_url = f'https://github.com/{my_github_name}/uo_puddles.git'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmp86ySdkr4z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "c13edfb7-fb73-489d-c707-f612d6da2c16"
      },
      "source": [
        "#this adds the library to colab so you can now import it\n",
        "!git clone $clone_url  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'uo_puddles'...\n",
            "remote: Enumerating objects: 213, done.\u001b[K\n",
            "remote: Counting objects: 100% (213/213), done.\u001b[K\n",
            "remote: Compressing objects: 100% (177/177), done.\u001b[K\n",
            "remote: Total 213 (delta 125), reused 64 (delta 33), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (213/213), 53.90 KiB | 303.00 KiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r8-rTeMkr5C"
      },
      "source": [
        "import uo_puddles.uo_puddles as up"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dagqgRpTyPl"
      },
      "source": [
        "#1. Spacy is the coolest\n",
        "\n",
        "My goal for the next part of the course is to take a more realistic view of text by looking at words. I want to get to counting words not letters like we did with the Titanic and tweets. Once we can count words, we can look at our next machine-learning algorithm, Naive Bayes.\n",
        "\n",
        "But before we get to that, I want to introduce you to a new library called spacy. It is built to deal with text so is a good fit for us. I'll show you how to import it - it is a little more complicated than pandas - and then how to start to make use of its features for word counting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vilI8ZGqZPDB"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVKhvwCTV6l9"
      },
      "source": [
        "That was easy enough!\n",
        "\n",
        "The problem is that spacy now needs a \"dictionary\" of words. It gives you the option of choosing your dictionary. I am going to choose one that contains about 20,000 words pulled from web pages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz-yQfqTyeMU",
        "outputId": "961919c5-a40c-41c3-c58a-0ec1001f3bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "#you first have to get colab to download it locally\n",
        "\n",
        "!python -m spacy download en_core_web_md\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051305 sha256=0ed2f32d1e9c833a538194e28b24787ef960ebe9150b1490a2cfccd029e786f5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6auz5nva/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD4w_Bdo-gZ1"
      },
      "source": [
        "#now you can import the dictionary and set up a parser\n",
        "\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()  #huh?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4pZlCxEWv_t"
      },
      "source": [
        "##What's up with that \"nlp =\" bit?\n",
        "\n",
        "For now, you can view nlp as a new parser function. We give it some text and it parses it into words. The tricky part is that it produces a new data type for holding those words. It is kind of like a list but not exactly. I'll show you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyMFbDH2ao7b"
      },
      "source": [
        "doc = nlp('the dfjaldfskj lives long.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q50fty9Pawu8"
      },
      "source": [
        "token1 = doc[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlVMnMAya5dY",
        "outputId": "60b6c63a-8803-4fbc-c3f7-953b0424d75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "token1.is_oov"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ljrghh4XIys"
      },
      "source": [
        "practice_sentence = 'spaCy is a relatively new package for “Industrial strength NLP in Python” developed by Matt Honnibal at Explosion AI.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6sJ9dABXYGv"
      },
      "source": [
        "doc = nlp(practice_sentence)  #doc is conventional name given to what you get back from a parse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp5ct_4yOzEn"
      },
      "source": [
        "##What is `doc` exactly?\n",
        "\n",
        "You may have heard the term *object-oriented programming* (OOP). Maybe only if you hang out with programmers. What doc is is an object. It might be easiest to view it as similar to a string or a list. It holds data. And it gives us ways to access that data. That's what an object does. I suppose it is not dissimilar to what pandas does, which gives us an object that acts like a table or spreadsheet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaAXr7CHP1nr"
      },
      "source": [
        "##spacy tokens\n",
        "\n",
        "When I ran the code `doc = nlp(practice_sentence)` above, spacy turned my string into what it calls *tokens*. A spacy token has what are called *properties* or *attributes*. One of those attributes is `text`. Check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z3zks_VnnYE"
      },
      "source": [
        "for i in range(len(doc)):\n",
        "  token = doc[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5XGCQ62XlFk",
        "outputId": "a614ce8c-47a6-4597-dc20-e7bde8dfb8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text)  #using the text attribute - gives me a string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy\n",
            "is\n",
            "a\n",
            "relatively\n",
            "new\n",
            "package\n",
            "for\n",
            "“\n",
            "Industrial\n",
            "strength\n",
            "NLP\n",
            "in\n",
            "Python\n",
            "”\n",
            "developed\n",
            "by\n",
            "Matt\n",
            "Honnibal\n",
            "at\n",
            "Explosion\n",
            "AI\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djqC5P4nI3Fm"
      },
      "source": [
        "It's a little confusing because up until now, we have seen a period followed by a method name followed by parens. For example, `my_string.lower()`. Now we see the period followed by a name and then no parens, e.g., `token.text`. The designers of spacy decided to do it this way. If you see a period then name then parens, you have to recognize it as a method. If you see the same minus the parens, you have to recognize it as an attribute.\n",
        "\n",
        "I imagine you are asking what the heck is all this stuff about methods and attributes? Where does that come from and do I need to know it? It comes from OOP. And you really should not have to know it. The designers of spacy could have just given you methods to use. I don't know why they did not. They are forcing you to learn special cases. Sorry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoKMIXkUQ_mu"
      },
      "source": [
        "##Let's look a little more closely at attributes\n",
        "\n",
        "First I'll grab the 2nd token and then look at some of its attributes. Note that I am only showing a small subset of all of the attributes available for a spacy token. For more, see https://spacy.io/api/token#attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6571m-dCRIvp"
      },
      "source": [
        "token1 = doc[1]  #can treat doc like a list and use list indexing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRKCm-uuoOBq",
        "outputId": "5b25f478-0d84-49bb-a342-fbb5ac18c47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "token1.lower()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9f2d2920468f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2tFtMQXX9Ph"
      },
      "source": [
        "print(token1.text)     #original word as appears in sentence\n",
        "print(token1.lemma_)   #lemmatized version\n",
        "print(token1.pos_)     #part of speech\n",
        "print(token1.tag_)     #verb, 3rd person singular present\n",
        "print(token1.dep_)     #references parse tree\n",
        "print(token1.shape_)   #2 chars both lower-case\n",
        "print(token1.is_alpha) #yes, consists of all alphanumeric characters\n",
        "print(token1.is_stop)  #yes, is a stopword\n",
        "print(token1.is_punct) #no, is not punctuation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCay7KENUZEG"
      },
      "source": [
        "##You can also see the parse tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjWuokd2Tl7M"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHoiEt8vVSM6"
      },
      "source": [
        "###How is this useful?\n",
        "\n",
        "There is a field in AI called Information Extraction. It is really what I would call old-school AI. It actually tries  to use a parse tree like above to extract information from a sentence. It can then find ways to represent what it extracts in a database of facts and relations.\n",
        "\n",
        "There are a couple of projects that try to build an AI mind by searching all of the web for information. They try to extract facts and relations until they can win Jeopardy :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eat8avCVNkr"
      },
      "source": [
        "##Spacy can also find entities\n",
        "\n",
        "See below. It recognizes a person's name and that *Explosion AI* is an organization. It is interesting that it does not recognize spacy (itself) as an entity!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbpejPCgUq9h"
      },
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgHj5wDEWOBd"
      },
      "source": [
        "###How is this useful?\n",
        "\n",
        "Again, if you are trying to extract information from a web page, it might be useful to know what are the important words you should pay attention to. The field is called *name-entity recognition*. spacy helps you find names and entities. It goes hand in hand with Information Extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkrHWM1xNCJ5"
      },
      "source": [
        "#Big (Huge) fork in the road here\n",
        "\n",
        "What spacy allows us to do is to explore the meaning of text (e.g., sentences) through what I would call **deep linguistics**. Using a mixture of parsing, syntactic structures, grammar rules, word-ordering rules, anaphora rules and statistics, deep linguistics attempts to extract meaning from text. And to actually generate text, e.g., for a chatbot.\n",
        "\n",
        "We are not following this path. We will use what I call **shallow linguistics**. The next machine learning algorithm we look at, Naive Bayes, simply uses word counts. Not the role the words play in a sentence, not the meaning of the words, not the context of the words. Just counts. So it is just one step up from our bag of characters.\n",
        "\n",
        "You may say, hey, deep sounds better than shallow to me. First answer. We will find that shallow can be quite impressive in the right setting. Second answer. Take some Linguistic courses to explore deep linguistics. This is a plug for our Linguistics Department :)\n",
        "\n",
        "I also ran across this recent article that argues that syntax structure is what we should be paying attention to. It was written by a linguist :)\n",
        "\n",
        "https://towardsdatascience.com/linguistic-rule-writing-for-nlp-ml-64d9af824ee8\n",
        "\n",
        "It's kind of cool because she uses spacy to make her arguments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzjq2-PaQYKE"
      },
      "source": [
        "##Deep == Shallow\n",
        "\n",
        "\n",
        "The third answer involves a bit of irony. The hot topic in machine learning today is **deep learning**. It turns out deep learning uses shallow linguistics! Maybe more precisely, it starts with shallow linguistics and learns  deep linguistics.\n",
        "\n",
        "I want to get to deep learning in this course. And set it up by looking at shallow linguistics. That said, the bleeding-edge research in NLP is starting to view the need for a combo of deep and shallow linguistics. The paper I reference above makes this argument as well. I am working on a project trying to spot errors in medical papers that attempts this balancing act."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZaSNwwyNAZ2"
      },
      "source": [
        "##OK, back to our original problem\n",
        "\n",
        "We will focus on using spacy's nlp parser to give us tokens which we can count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAuPW0XhZ5UJ"
      },
      "source": [
        "#2. Bring in next data set\n",
        "\n",
        "I found kind of a fun text data set. Each row is a sentence taken from one of three gothic authors: Mary Wollstonecraft Shelley (MWS), Edgar Allan Poe (EAP), and H.P. Lovecraft (HPL). What makes these authors gothic? Because my literary colleagues told me so!\n",
        "&nbsp;<br>&nbsp;<br>\n",
        "<center>\n",
        "<img src='https://www.thegreatcoursesdaily.com/wp-content/uploads/2017/03/Mary-Shelly-Featured-Image-LARGE.jpg' height='150' >\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src ='https://www.riverwoodwinery.com/uploads/1/2/4/1/124189002/s593933943352694433_p9_i1_w593.jpeg' height='150'>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src='https://wpcdn.us-midwest-1.vip.tn-cloud.net/www.rimonthly.com/content/uploads/2017/05/13135413/4b8e1f244f4e0e42bf881efabd19aa12-AU5EC102CUR.jpg' height='150'>\n",
        "</center>\n",
        "\n",
        "Here is the problem I will set out for us: given a sentence taken randomly from the works of author X (one of the 3), predict X. Let's bring in the data and check it out.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74AsQuj4bvej"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K713HblbzsA",
        "outputId": "ea2a33cb-48f6-4b4c-ace0-6f1b5613fc34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "gothic_sentences = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQqRwyE0ceZREKqhuaOw8uQguTG6Alr5kocggvAnczrWaimXE8ncR--GC0o_PyVDlb-R6Z60v-XaWm9/pub?output=csv',\n",
        "                          encoding='utf-8')\n",
        "len(gothic_sentences) #how many sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpZqCAKyD_Uu",
        "outputId": "49d2f1f9-497d-4d3a-ab47-492c77e37479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "gothic_sentences.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bjMiQ9UDpqr"
      },
      "source": [
        "The next bit of code tells pandas to show all of the text when displaying the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Mf50alDxa6"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)  #None forces all of sentence to be shown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZw8hYOQb33w"
      },
      "source": [
        "gothic_sentences.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmDssfVaaBs"
      },
      "source": [
        "##Not too different from tweets\n",
        "\n",
        "We have text, now a sentence instead of a tweet. We have a label, now one of 3 authors instead of hate/no-hate. I kind of like this problem because it branches out from binary prediction. We now have 3 different possible values for a label. No problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxpbT6xxcwDY"
      },
      "source": [
        "##My general plan\n",
        "\n",
        "1. We will want to do holdout as usual. So shuffle table and split into training and testing.\n",
        "\n",
        "2. Build what is called a \"bag of words\" from the training table.\n",
        "\n",
        "3. Introduce Naive Bayes, which will make use of the bag of words.\n",
        "\n",
        "4. Evaluate on testing data.\n",
        "\n",
        "Whew. Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aedN9YTZdaFl"
      },
      "source": [
        "#3. Compute holdout data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g3Q9PBZZ_G5"
      },
      "source": [
        "our_seed = 1234  #if we all use this we should get same random data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoT_UF0hhdmA"
      },
      "source": [
        "import numpy as np  #powerful library for manipulating data\n",
        "rsgen = np.random.RandomState(our_seed)  #we are only going to use numpy's random number generator for now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vStMqeJhiPEh"
      },
      "source": [
        "shuffled_table = gothic_sentences.sample(frac=1, random_state=rsgen).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vcUvTIfnoyd",
        "outputId": "06a5aeff-6d43-4455-ae67-d46a75985ade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "len(shuffled_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRLat0NJmR7S",
        "outputId": "f3e284fa-a4f5-40fe-dfd5-2ef056d6dd97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "19579*.7  #split point"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13705.3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxslltTfmbks"
      },
      "source": [
        "training_table = shuffled_table[:13705].reset_index(drop=True)  #.7\n",
        "testing_table = shuffled_table[13705:].reset_index(drop=True)   #.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXfPhQ4hQFPM"
      },
      "source": [
        "Now pull the sentences into list of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSCKZHlqnITo"
      },
      "source": [
        "training_text = training_table['text'].to_list()\n",
        "training_authors = training_table['author'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBpfNZXS-1E0"
      },
      "source": [
        "testing_text = testing_table['text'].to_list()\n",
        "testing_authors = testing_table['author'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXWQ9O7Wum0b"
      },
      "source": [
        "#4. Perspective shift!\n",
        "\n",
        "If we were going to reuse our KNN algorithm here, I would need to replace the 26 letters with what? With words. Let's say I find 20K unique words in the training text. Then I would have 20K columns in my table, one for each unique word as opposed to one for each letter. This is certainly doable in pandas. It can handle 20K columns. Might have to think about how to print such a table! But it is doable. So if you want to use KNN with gothic author sentences then (a) find unique words (we got to skip this step because we knew the 26 letters), create the columns (you've done this), and do the counting (you've also done this).\n",
        "\n",
        "We are going to go in a different direction at this point. We will actually get back to something that is similar to KNN with words later in the quarter. But for now we are going to build a variation of what is called a word-bag. It will replace the matrix (e.g., all_passenger_matrix) se used in KNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5-EDmvpluRh"
      },
      "source": [
        "#5. Building a word bag\n",
        "\n",
        "To use Naive Bayes, we need a word-bag. I think it might be easiest to show you.\n",
        "\n",
        "I'll start with an empty word-bag table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G77MSxTmKI9"
      },
      "source": [
        "word_table = pd.DataFrame(columns=['word', 'EAP', 'MWS', 'HPL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J4NS8FqmKJA"
      },
      "source": [
        "word_table.head()  #empty table so far"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjlq4HsomfA1"
      },
      "source": [
        "Now I'll add a row. I happen to know that EAP used the word *indefinite* in a sentence so I want to add a row for that. Here is row I want to add as a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAB9CoH9m7Pr"
      },
      "source": [
        "row_list = ['indefinite', 1, 0, 0]  #matches columns [word, EAP, MWS, HPL]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Z5f4acm12t"
      },
      "source": [
        "word_table.loc[0] = row_list  #in essence, append a new row and fill with row_list values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fco5Csj_peoB"
      },
      "source": [
        "word_table.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxNVRc2UqAI4"
      },
      "source": [
        "##Puddles to the rescue\n",
        "\n",
        "There is a function in puddles that will do what we had to do by hand above. Check this out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zicysu6oVBc"
      },
      "source": [
        "Start over with clean table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "394xO829oX3f"
      },
      "source": [
        "word_table = pd.DataFrame(columns=['word', 'EAP', 'MWS', 'HPL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZznBqh6Co-mk"
      },
      "source": [
        "word_table.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzSho-7foarS"
      },
      "source": [
        "Record we saw indefinite used by EAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYrQhORpqLsM"
      },
      "source": [
        "up.update_gothic_row(word_table, 'indefinite', 'EAP')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1gKGHtupcVs"
      },
      "source": [
        "Let's say we saw EAP use it in another sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sicPvEKepiBb"
      },
      "source": [
        "up.update_gothic_row(word_table, 'indefinite', 'EAP')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyQGsEvxeIB"
      },
      "source": [
        "In words, the update_gothic_row function takes a word and an author. It first checks to see if the word is already in the table. If it is not, it creates a row for it.\n",
        "\n",
        "It then finds the column that goes with the author and increments the value by 1.\n",
        "\n",
        "If we can figure out how to call the function over and over, we should end up with a table of words along with a count of how many times each author used a word in a sentence. Cool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BF4bw1-p3Ef"
      },
      "source": [
        "##General strategy\n",
        "\n",
        "Here is strategy we can use to build up our word bag:\n",
        "\n",
        "1. Look at sentence at row 0.\n",
        "\n",
        "2. Get author as well.\n",
        "\n",
        "3. Parse sentence into words using `spacy`.\n",
        "\n",
        "4. For each word, update our word_table appropriately using `update_gothic_row`.\n",
        "\n",
        "5. Repeat steps 1-4 for all rows in the training table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRxPt5XYdpST"
      },
      "source": [
        "#6. Introduce new idea - stopwords\n",
        "\n",
        "I'm ready to build my bag of words but we need to think about a new idea: *stop words*. In working with text, researchers (computer scientists, linguists, literary scholars, etc.) have decided that certain English words are so common that they carry no value. They have become to be called stop words. Why this term? It's a little obscure to me. I think \"no value words\" would be better.\n",
        "\n",
        "spacy gives us a list of stop words so we will use that. But there is no universal agreement of what words should go in the list. And you can certainly build your own list of stop words to use.\n",
        "\n",
        "**BTW**: I could see bias creeping in here. I bet I could engineer a set of words to be removed that could slant our models to always predict a specific author.\n",
        "\n",
        "**BTW2**: Removing stop words is the first step down the shallow linguistics path. By eliminating words, we are losing the ability to ascertain syntactic structure. And using syntactic structure is one of bedrocks of deep linguistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72zdg8Rq1YT_"
      },
      "source": [
        "sent0 = training_text[0]\n",
        "sent0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok9wAQj_1jev"
      },
      "source": [
        "doc0 = nlp(sent0.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q08w8PiosSHS"
      },
      "source": [
        "So in words what I want for step 4 is \"For each word, update our word_table **appropriately** using update_gothic_row\". So we need to take into account whether we have a word (is_alpha) and whether it is a stop word (is_stop). Something like this:\n",
        "\n",
        "4. For each word, **if** the word is alpha and the word is not a stop word, **then** update our word_table appropriately using `update_gothic_row`.\n",
        "\n",
        "We will need some new Python syntax to carry this out. In particular, we will need a way to make decisions in the code, itself. The jargon for what we need is a *conditional*. We will run some code on the \"condition\" that are wishes are met. If they are not met, we will not run the code.\n",
        "\n",
        "Let's try it out with a few examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDtmMnLHhGyK"
      },
      "source": [
        "#7. If statement is what we will use\n",
        "\n",
        "It is a conditional in Python.\n",
        "\n",
        "Check it out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc4v6OX2zFan"
      },
      "source": [
        "word_table.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuTYEgLUt0PV"
      },
      "source": [
        "token = doc0[1]\n",
        "print(token.text)\n",
        "author = 'EAP'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyfBPgJlt7kG"
      },
      "source": [
        "if token.is_alpha and not token.is_stop:\n",
        "  up.update_gothic_row(word_table, token, author)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxSQCtp3utdW"
      },
      "source": [
        "word_table.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TllQphNGzOG2"
      },
      "source": [
        "What we want. The word *indefinite* is a word and is not a stop word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ds6DKWrKoA"
      },
      "source": [
        "Let's break this apart.\n",
        "\n",
        "* **if**. This starts a conditional. When Python sees it, it expects a certain form for what is to follow.\n",
        "\n",
        "* **token.is_alpha and not token.is_stop**. This is the condition. It must evaluate to True or False. Let's break it apart.\n",
        "\n",
        "* **token.is_alpha**. This evaluates to True or False. Good.\n",
        "\n",
        "* **token.is_stop**. This also evaluates to True or False.\n",
        "\n",
        "* **not**. This is logical negation. `not True` evaluates to `False`; `not False` evaluates to `True`.\n",
        "\n",
        "* **and**. This is logical conjunction. For it to be True, both of its conditions must be True.\n",
        "\n",
        "* **:** The annoying colon. It brackets the condition. So what is between the **if** and the **:** must be a logical expression that evaluates to `True` or `False`.\n",
        "\n",
        "So again in words, if the token is alphabetic and it is not a stop word, then do something.\n",
        "\n",
        "###The \"then clause\"\n",
        "\n",
        "The jargony name for the indented lines that come after the colon is the *then clause*. In the case above it is the line   `up.update_gothic_row(word_table, token, author)`. We only execute this line if the logical condition is True. Otherwise we skip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pudgOwh0u5TB"
      },
      "source": [
        "token = doc0[0]  #an\n",
        "print(token)\n",
        "author = 'EAP'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qex_woTvu5TG"
      },
      "source": [
        "if token.is_alpha and not token.is_stop:\n",
        "  up.update_gothic_row(word_table, token, author)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgUY6om-u5TJ"
      },
      "source": [
        "word_table.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYjrpuvDzbZU"
      },
      "source": [
        "Nothing was added to the table. Why? Because `token.is_stop` is true and hence `not token.is_alpha` is false."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqe-NSlAu_fq"
      },
      "source": [
        "token = doc0[-1]  #period\n",
        "print(token)\n",
        "author = 'EAP'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Q6aw5yu_fu"
      },
      "source": [
        "if token.is_alpha and not token.is_stop:\n",
        "  up.update_gothic_row(word_table, token, author)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWswmIeiu_fw"
      },
      "source": [
        "word_table.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQtSykTWz1Ox"
      },
      "source": [
        "Nothing was added to the table. Why? Because `token.is_alpha` is false."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFYBzBgxqMUb"
      },
      "source": [
        "##We are good for now\n",
        "\n",
        "if-statements can come in more complex varieties. But the form above should work for us for the present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIYiUrSIU0Hk"
      },
      "source": [
        "#8. New concept: nested-loops\n",
        "\n",
        "We need one more thing before getting to your first assignment. In words, here is what you need to do:\n",
        "\n",
        "1. Go through each sentence `S` in `training_text`.\n",
        "2. Go through each token `T` in `S`.\n",
        "3. Update `word_table` accordingly with `T`.\n",
        "\n",
        "We have nested-loops. The outer loop is step 1. The inner loop is step 2.\n",
        "\n",
        "Let's practice with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLG9HTbpWgqA"
      },
      "source": [
        "Let's say I have this list of lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZITvRtc7WmGf"
      },
      "source": [
        "list_of_chars = [['a', 'b', '3'], ['c', '4'], ['5', '6', 's', 'd']]\n",
        "list_of_chars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1HFxlv_W3M_"
      },
      "source": [
        "What I want is a new list that is just the letters. So this:\n",
        "\n",
        " `['a', 'b', 'c', 's', 'd']`.\n",
        "\n",
        " I'm going to sneak up on the solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyBAny9-W9gs"
      },
      "source": [
        "letter_list = []\n",
        "\n",
        "for i in range(len(list_of_chars)):\n",
        "  inner_list = list_of_chars[i]  #a list\n",
        "  print(inner_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOMSifbzXO88"
      },
      "source": [
        "letter_list = []\n",
        "\n",
        "for i in range(len(list_of_chars)):\n",
        "  inner_list = list_of_chars[i]  #a list\n",
        "  \n",
        "  for j in range(len(inner_list)):\n",
        "    a_char = inner_list[j]  #a char\n",
        "    print(a_char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj6YrHuvXjkZ"
      },
      "source": [
        "letter_list = []\n",
        "\n",
        "for i in range(len(list_of_chars)):\n",
        "  inner_list = list_of_chars[i]  #a list\n",
        "  \n",
        "  for j in range(len(inner_list)):\n",
        "    a_char = inner_list[j]  #a char\n",
        "    if a_char.isalpha():\n",
        "      print(a_char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLbKJ9bm08bo"
      },
      "source": [
        "Full answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj69bzuRX8Rl",
        "outputId": "26e0f489-508d-4e45-813c-8c1e08121c47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "letter_list = []\n",
        "\n",
        "for i in range(len(list_of_chars)):\n",
        "  inner_list = list_of_chars[i]  #a list\n",
        "  \n",
        "  for j in range(len(inner_list)):\n",
        "    a_char = inner_list[j]  #a char\n",
        "    if a_char.isalpha():\n",
        "      letter_list.append(a_char)\n",
        "\n",
        "letter_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d949de6b39bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mletter_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0minner_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_of_chars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'list_of_chars' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIUCCyPs659a"
      },
      "source": [
        "#Assignment 1\n",
        "\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "I think you are good to go. Build word_bag using the `training_text` and `training_authors` lists. Use the conditional above to make sure you only include alpha words and non-stop-words.\n",
        "\n",
        "Warning: this took me 5 minutes of wall-time to compute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yppaash4YtPE"
      },
      "source": [
        "I'll try to get you started. Below is the \"new list\" we are building. But it is a table, not a list. And instead of appending, you will use the `up.update_gothic_row` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYuGlnvlyTlN"
      },
      "source": [
        "%%time\n",
        "#training_text and training_authors are your 2 old lists\n",
        "\n",
        "word_table = pd.DataFrame(columns=['word', 'EAP', 'MWS', 'HPL'])  #this is the \"new list\", actually a table\n",
        "\n",
        "#your nested for-loops below\n",
        "for i in range(len(training_text)):\n",
        "  s = training_text[i].lower()\n",
        "  doc = nlp(s)\n",
        "  author = training_authors[i]\n",
        "\n",
        "  for j in range(len(doc)):\n",
        "    token = doc[j]\n",
        "    if token.is_alpha and not token.is_stop:\n",
        "      up.update_gothic_row(word_table, token.text, author)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN2cyBmTga8W",
        "outputId": "55c04296-31ab-4a69-a0c4-a5f681739f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "len(word_table)  #21552 words. Yikes."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21552"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dllHs9_uE4M"
      },
      "source": [
        "##We found 21552 unique non-stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpzPVmHkvt7S",
        "outputId": "ecc90c37-a40e-4c50-bcf7-c8ee76cf5c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "word_table.head()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>EAP</th>\n",
              "      <th>MWS</th>\n",
              "      <th>HPL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>indefinite</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sense</td>\n",
              "      <td>44</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>awe</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sight</td>\n",
              "      <td>31</td>\n",
              "      <td>32</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>navigators</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         word EAP MWS HPL\n",
              "0  indefinite  10   0   2\n",
              "1       sense  44  18  27\n",
              "2         awe  12   6   4\n",
              "3       sight  31  32  52\n",
              "4  navigators   1   1   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axBCtsndZlO_"
      },
      "source": [
        "Match mine:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/1eycqs3lzo7hq1n/Screenshot%202020-04-18%2013.39.56.png?raw=1' height=200>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5sxeAY7U1ZS",
        "outputId": "299edcb8-470d-4695-9d40-df8dc0aa5772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "word_table.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>EAP</th>\n",
              "      <th>MWS</th>\n",
              "      <th>HPL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21547</th>\n",
              "      <td>differential</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21548</th>\n",
              "      <td>nobly</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21549</th>\n",
              "      <td>conferred</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21550</th>\n",
              "      <td>uniformly</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21551</th>\n",
              "      <td>branded</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               word EAP MWS HPL\n",
              "21547  differential   1   0   0\n",
              "21548         nobly   0   1   0\n",
              "21549     conferred   0   1   0\n",
              "21550     uniformly   1   0   0\n",
              "21551       branded   0   1   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68EAvrYCBfM1"
      },
      "source": [
        "Match mine:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/vzl0aasr2g4c1r6/Screenshot%202020-04-18%2013.41.21.png?raw=1' height=200>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfx5nLQ6VohS"
      },
      "source": [
        "##Sort on word\n",
        "\n",
        "pandas gives you a method to do sorting based on a column. We will use the word column to sort.\n",
        "\n",
        "Note we could also sort on one of the authors to see which words they used most frequently. And which words they never used. Could be interesting in its own right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSf7xgzdVrhZ"
      },
      "source": [
        "sorted_word_table = word_table.sort_values(by=['word'])\n",
        "sorted_word_table = sorted_word_table.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ5vNzFMXQtn",
        "outputId": "d4c7f7bd-48e7-49d3-9ebb-12a7843bb0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "sorted_word_table.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>EAP</th>\n",
              "      <th>MWS</th>\n",
              "      <th>HPL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ab</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aback</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abaft</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>abandon</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>abandoned</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        word EAP MWS HPL\n",
              "0         ab   1   0   0\n",
              "1      aback   2   0   0\n",
              "2      abaft   0   1   0\n",
              "3    abandon   3   1   2\n",
              "4  abandoned   8   4   8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBYOlqsme_QK"
      },
      "source": [
        "##Choose new index\n",
        "\n",
        "Up until now we have been using numercial indices, e.g., 0, 1, 2, etc. I think it will make our life easier to choose the word column as the index. The word column has unique values so gives us an unamibuous index. And I'll show you a way to use it to quickly look things up.\n",
        "\n",
        "pandas has a method for choosing a column to make as index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4NlnZeSfXHE",
        "outputId": "20869a4b-1b57-4e82-ce7e-3512825e8e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "sorted_word_table = sorted_word_table.set_index('word')  #set the word column to be the table index\n",
        "sorted_word_table.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EAP</th>\n",
              "      <th>MWS</th>\n",
              "      <th>HPL</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ab</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aback</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abaft</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandon</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandoned</th>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          EAP MWS HPL\n",
              "word                 \n",
              "ab          1   0   0\n",
              "aback       2   0   0\n",
              "abaft       0   1   0\n",
              "abandon     3   1   2\n",
              "abandoned   8   4   8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk8BXVD6flua"
      },
      "source": [
        "The hint that we have `word` as index is in the way it now prints a bit below the column names. Subtle.\n",
        "\n",
        "Here is the way I can use it. It is a twist on `iloc`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W__Q3Jpf3jg",
        "outputId": "de494b32-4bce-4304-a4b6-55ab2ddab2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "sorted_word_table.loc['indefinite'].tolist()  #so loc instead of iloc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 0, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-L9KPSi-fj3"
      },
      "source": [
        "##Even better\n",
        "\n",
        "I can use loc to get at a specifc column in a row. Can't do that with iloc. Check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbVXAXvn-nzE",
        "outputId": "214f4b84-6037-4b14-a6bd-8af5bb1a7f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "sorted_word_table.loc['indefinite', 'EAP']  #give index first then column name to get a cell"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoRs7BI0VPM0"
      },
      "source": [
        "#Write it out!\n",
        "\n",
        "Don't want to have to keep building the bag everytime we restart this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRMNoMP3_ViF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAhLfOgTPqez"
      },
      "source": [
        "with open('/content/gdrive/My Drive/word_bag_s20.csv', 'w') as file:\n",
        "    sorted_word_table.to_csv(file, index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w59lzClx9ujw"
      },
      "source": [
        "##Two ways to read it back in\n",
        "\n",
        "First is to read it directly off your drive. If you do this, I will not be able to run your code given I do not have access to your Google Drive. You will also have to run the `drive.mount` code each time. I'd prefer you did not use this method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN0X6llptRPV"
      },
      "source": [
        "with open('/content/gdrive/My Drive/word_bag_s20.csv', 'r') as file:\n",
        "    sorted_word_table = pd.read_csv(file, dtype={'word':str}, encoding='utf-8',\n",
        "                                    index_col='word', na_filter=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hadtnfkS96nm"
      },
      "source": [
        "##The second way is to grab url and use that.\n",
        "\n",
        "Then I can run your code. And don't have to mount Google Drive. Please use this if you can."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyAbmgU6P4S1"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW0vmVc02Vfw"
      },
      "source": [
        "#url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQR107nAfeU_z-p6sUv3yhnti9vNsklgXsm2RXAExQBHPUE3APm32qMQxTuYCEBbSz09MCVx-rnOXGb/pub?output=csv'\n",
        "url = 'https://drive.google.com/open?id=1A6WfPV6Z-vABCX87Yh4us1IRHlbbt8uA '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTy80-RHv8sN"
      },
      "source": [
        "sorted_word_table = pd.read_csv(url, dtype={'word':str}, encoding='utf-8',\n",
        "                                index_col='word', na_filter=False)\n",
        "sorted_word_table = sorted_word_table.rename(index={'TRUE': 'true', 'FALSE': 'false'}) #need this because of bug in reading from url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6j6fjlk2zsP",
        "outputId": "f01276d0-340e-4112-e60c-d65932c91a43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "sorted_word_table.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EAP</th>\n",
              "      <th>MWS</th>\n",
              "      <th>HPL</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ab</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aback</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abaft</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandon</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abandoned</th>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           EAP  MWS  HPL\n",
              "word                    \n",
              "ab           1    0    0\n",
              "aback        2    0    0\n",
              "abaft        0    1    0\n",
              "abandon      3    1    2\n",
              "abandoned    8    4    8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymK0iXhulH4R"
      },
      "source": [
        "#Will start here on Wednesday"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J0slaJrtezw"
      },
      "source": [
        "#9. Another algorithm: Naive Bayes\n",
        "\n",
        "\n",
        "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQNRXY3N9qg2REofs9Li1VyLGTzKNZXvNuHhi-jISIbs1w9Uf8d&usqp=CAU' height=100>\n",
        "\n",
        "Our problem is similar to your tweet midterm problem. Given a sentence/tweet, predict the author. We used KNN on a bag of characters. The twist is that I am going to reform the question in terms of probabilities using a method called *Naive Bayes*. I'll tell you what is naive about it later. As reminder, we will no longer be using a matrix-based approach that looks at lists of features represented as vectors. Instead we will use counts of things. Where do these counts come from? Your word bag in Assignment 1 :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpybfiI47r9c"
      },
      "source": [
        "#First, here is the basic Bayes Theorem\n",
        "\n",
        "We can use it as a basis to get to Naive Bayes.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/efpfgkenlit9rk1/Screenshot%202020-04-23%2009.42.25.png?raw=1' height=200>\n",
        "\n",
        "The jargony terms for the various pieces are as follows:\n",
        "\n",
        "* P(A|B): The *posterior* probability.\n",
        "\n",
        "* P(B|A): The *liklihood* or *conditional probability*.\n",
        "\n",
        "* P(A): A *marginal* or *prior* probability.\n",
        "\n",
        "* P(B): A *marginal* or *prior* probability.\n",
        "\n",
        "Before digging into what all this means, I'd like to get to the expansion of basic Bayes to what we will be using: Naive Bayes. The difference is in terms of B above. B is meant to represent a single piece of evidence we have collected. I want to expand B into mulitple pieces of evidence. That gets us to Naive Bayes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp10lmrI7p5L"
      },
      "source": [
        "#Naive Bayes theorem\n",
        "\n",
        "My apologies on switching notation on you. I'm using A and B above and now O (capital o) and E. Once you make that mental switch, you should see that our single B above becomes a collection of Es, e.g., E1, E2, etc. This says we have multiple pieces of evidence at our disposal.\n",
        "First, here is the formula we want to compute. Looks different than Geometery or Trig, right? New syntax and concepts.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/gstzvvtvh9b39o8/bayes.png?raw=1'>\n",
        "\n",
        "Let me break out the notation.\n",
        "\n",
        "1. The `O` stands for one of the \"classes\" we are using. We have 3 classes, right? EAP, MWS, HPL. So the O stands for one of them. It is a parameter to the formula. We have to fill it in before we start computing it.\n",
        "It may be best to view `P(O|E1, E2 ...)` as a Python function call. We have to fill in values for O and all the Es.\n",
        "\n",
        "2. This implies that we really have 3 separate instances of this formula, right?\n",
        "<pre>\n",
        "P(EAP|E1, E2 ...) = ...\n",
        "P(MWS|E1, E2 ...) = ...\n",
        "P(HPL|E1, E2 ...) = ...\n",
        "</pre>\n",
        "Kind of confusing. What we want to do is compute all 3 formulas above to get 3 different values. The 3 values we get are the probabilities that the sentence belongs to each of the 3 respective classes. The way the formula is set up, these probablities are normalized to add to 1.\n",
        "\n",
        "3. What are the values for `E1, E2`, etc.? They are the words in the sentence. More precisely, they are the spacy tokens that meet `is_aplha` and `not is_stop`. So we parse our sentence in normal fashion to get the tokens and view these tokens as E1, E2, etc. You will see that we don't care what order the tokens come in. We are ignoring their context of usage. They are just a collection (bag) of tokens. You can assign the Es to them in any fashion you like.\n",
        "\n",
        "4. It might be best to use a real example at this point. Let's say we are looking at `P(EAP|E1, E2 ...)` and we see the token/word *indefinite* in the parse of the sentence. Let's say E1 = indefinite. On the right-hand side I see this: `P(E1|O)` which maps to `P(indefinite|EAP)`. What this asks informally is \"what is the probablity of seeing the word indefinite in a sentence that EAP wrote?\" In other words, out of all the sentences that EAP wrote, how many times did indefinite show up? Could be zero!\n",
        "\n",
        "5. What else is on the right-hand side? How about this: `P(O)`, or in our case `P(EAP)`. This asks what the probability is that EAP wrote a sentence out of all the (training) sentences we have seen.\n",
        "\n",
        "7. One more: `P(E1)` or `P(indefinite)`. Out of all the sentences written across all 3 authors, how many included `indefinite`?\n",
        "\n",
        "8. Back to the left-hand side. We can read `P(EAP|E1, E2 ...)` as what is the probability that EAP wrote this sentence given the tokens in the sentence, where we label the tokens E1, E2, etc. The Es are our \"evidence\". Given the evidence, we are being Bayesian sleuths, trying to ferret out the culprit (a bit of a strained metaphor).\n",
        "\n",
        "##Now what?\n",
        "\n",
        "We have to figure out how to actually get all these values and do the products and division. I am going to compute a few things first that should help us later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ID_tXT6VeX"
      },
      "source": [
        "##Probabilites of authors P(O)\n",
        "\n",
        "These will not change - they are constant. So we only have to compute them once then can reuse them in the formula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW7N9lHi6cu1",
        "outputId": "3e5c7557-267d-48dc-ea4c-34bfe6918508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "n = len(training_text)\n",
        "n  #13705"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13705"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDVl_p365fn",
        "outputId": "c231b0b5-fdbf-4e1a-c122-2a92423d6b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "eap_count = training_authors.count('EAP')\n",
        "eap_count  #5470"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5470"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "d64f25ec-033d-4f9f-c483-293a31ba5dce",
        "id": "ayvrOH1i7BSY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "mws_count = training_authors.count('MWS')\n",
        "mws_count  #4278"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4278"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "878cd4ec-bb08-4fc5-a358-9b013ffa9bfd",
        "id": "T0Qz9w-O7Ban",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "hpl_count = training_authors.count('HPL')\n",
        "hpl_count  #3957"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "02926a0d-dc28-47ba-ba13-88bdfd3306d4",
        "id": "x0_Q7x_f7O71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "p_eap = eap_count/n  #P(EAP)\n",
        "p_eap  #0.39912440715067493"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39912440715067493"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "a5094f1c-c71b-4e33-9c9d-552a578c9282",
        "id": "kZYzQKIr7bn0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "p_mws = mws_count/n  #P(MWS)\n",
        "p_mws  #0.31214885078438526"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.31214885078438526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "6745d357-82fd-42bd-f5a6-536850861dcb",
        "id": "_WR1Ec4q7bs8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "p_hpl = hpl_count/n  #P(HPL)\n",
        "p_hpl  #0.2887267420649398"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2887267420649398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZqj3zto51n0"
      },
      "source": [
        "##Compute P(indefinite|EAP)\n",
        "\n",
        "If we can find a general strategy for that, then we can apply it to any form of P(Ei|O), right?\n",
        "\n",
        "##Big assumption\n",
        "\n",
        "I am going to assume that no word in `sorted_word_bag` appears more than once in any sentence. I actually did a check on this and found no cases where it was broken. So I don't think it is an unreasonable assumption. And I could make sure it is true by asking you to add a bit more code to your assignment 1 loop (but I won't :)\n",
        "\n",
        "Given this assumption, I think we have the answer below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-tZwbnU82cA",
        "outputId": "b4c6780c-dac3-49fa-9ffb-fd048a8f90be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#count the number of sentences eap wrote. Out of those, how many used indefinite?\n",
        "p_indefinite_eap = sorted_word_table.loc['indefinite', 'EAP']/eap_count\n",
        "p_indefinite_eap  #0.0018281535648994515"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0018281535648994515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iCVzcOL96fn"
      },
      "source": [
        "##How about P(indefinite)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYH9_9209_D8",
        "outputId": "384ee1e9-e9df-421f-f223-d836935c3322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#count the total number of sentences. Count the total use of indefinite? What is the percentage?\n",
        "p_indefinite = sum(sorted_word_table.loc['indefinite'].tolist())/n\n",
        "p_indefinite  #0.0008755928493250639"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0008755928493250639"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQrhUqgq_bmJ"
      },
      "source": [
        "sum([2,4,5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w67ZczAtAvx0"
      },
      "source": [
        "#Assignment 2\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "Compute P(O|E1, e2, ...) for a specific example. I'll set it up for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiTivo4Ay-lE"
      },
      "source": [
        "##A note on new notation\n",
        "\n",
        "The probablities we compute are going to get very small, i.e., while still positive, very close to zero.\n",
        "\n",
        "For these small values, you will see something like `7.296607077708865e-05`. What's up with that `e-05`? It is a type of exponentiation using the math constant `e`. Let's see what it looks like in code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lUr89L11CVA",
        "outputId": "0d5824f2-a107-4f28-a955-5dee82313fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import math\n",
        "\n",
        "math.exp(-5)  #same as e-05"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.006737946999085467"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x15imRTA1N1o"
      },
      "source": [
        "This is the important piece to remember."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvkGHno61R1_",
        "outputId": "7e082ec2-bc3c-4e36-821d-676048f2472c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "math.exp(-5) > math.exp(-6) #the bigger the negative, the smaller the number"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92-bcPdN1XvQ"
      },
      "source": [
        "So the larger the negative number, the smaller the number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ80d2hUA8Ex"
      },
      "source": [
        "##Step 1.\n",
        "\n",
        "Let's choose a sentence. How about this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqbfyKmMBDZc",
        "outputId": "7d517cd3-d3dc-4b2a-ea2a-426d50fe45d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "a_sentence = training_text[2]\n",
        "a_sentence  #\"Why don't you laugh at Oliver's grandfather, who won't ride in a motor?\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't you laugh at Oliver's grandfather, who won't ride in a motor?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e56cDFcEBpnO"
      },
      "source": [
        "###Now build a list of words\n",
        "\n",
        "Use spacy to get your tokens from `a_sentence` and store them in doc. Then go through doc and add token.text to a new list. Remember to meet the constraints: `is_alpha` and `not is_stop`.\n",
        "\n",
        "Remember to append `token.text` and not just `token`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBt4zk64CW-L"
      },
      "source": [
        "#your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UaZ86s_EZEV",
        "outputId": "4a82442c-b831-4b8d-ac5e-e8a6daf202b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "e_list #['laugh', 'oliver', 'grandfather', 'wo', 'ride', 'motor']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['laugh', 'oliver', 'grandfather', 'wo', 'ride', 'motor']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oljBkk_5E0Ic"
      },
      "source": [
        "For simplicity, let's assume that e_list[0] is E1, e_list[1] is E2, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU6rZD-qE_oo"
      },
      "source": [
        "##Step 2.\n",
        "\n",
        "Assume that we use 'EAP' for 'O'. Compute the values P(Ei|EAP) using `e_list` to get your tokens and store those values in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkDRp_zFSMH"
      },
      "source": [
        "#your code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whL8BwDlFtlc",
        "outputId": "ad51ee01-cd3f-474c-9bfe-ae236a2cba0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(p_ei_eap_list)  #[0.0009140767824497258, 0.0, 0.0, 0.0021937842778793418, 0.0009140767824497258, 0.0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0009140767824497258, 0.0, 0.0, 0.0021937842778793418, 0.0009140767824497258, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqY513P1GCpQ",
        "outputId": "e1e49738-2acd-49b6-aeef-2cf29eed41f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "list(zip(e_list, p_ei_eap_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('laugh', 0.0009140767824497258),\n",
              " ('oliver', 0.0),\n",
              " ('grandfather', 0.0),\n",
              " ('wo', 0.0021937842778793418),\n",
              " ('ride', 0.0009140767824497258),\n",
              " ('motor', 0.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8wiPXjGLIJ"
      },
      "source": [
        "What does that tell us? Looks like eap never used the word *oliver*, *grandfather* or *motor*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiZzViPjHBet"
      },
      "source": [
        "##Step 3.\n",
        "\n",
        "Compute P(Ei). Create a list of the values for each word in e_list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2kQjEPJHO2e"
      },
      "source": [
        "#your list here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_a01pKtHmpW",
        "outputId": "0db7aacf-d30c-4291-ad37-09bac1cdcdcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(p_ei_list)  #[0.0010944910616563297, 7.296607077708865e-05, 0.0016052535570959504, 0.0015322874863188617, 0.0008026267785479752, 0.00036483035388544326]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0010944910616563297, 7.296607077708865e-05, 0.0016052535570959504, 0.0015322874863188617, 0.0008026267785479752, 0.00036483035388544326]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2yQj9WaH1da"
      },
      "source": [
        "#Step 4.\n",
        "\n",
        "We need P(EAP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGSb16HtH6ks",
        "outputId": "050226f2-6563-4b21-97dd-04ce28493038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "p_eap  #we already have it :)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39912440715067493"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aDaKK8MH_zV"
      },
      "source": [
        "#Step 5.\n",
        "\n",
        "Glue all this together. Unfortunately, Python does not have the multiplication equivalent to the `sum` function. So I am going to give you one. And at the same time, get you to think a bit about how you can write your own functions! I'll let you try your hand with this later in the quarter.\n",
        "\n",
        "BTW: we have seen that greek sigma stands for sum. The greek pi stands for product.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/xtu8yvf2xklze1f/Screenshot%202020-04-22%2014.59.49.png?raw=1' height=100>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqSalY5DIR6G"
      },
      "source": [
        "#I am defining a new function float_mult - cool.\n",
        "\n",
        "def float_mult(number_list: list) -> float:\n",
        "  assert isinstance(number_list, list), f'number_list should be a list but is instead a {type(number_list)}'\n",
        "  assert all([isinstance(item, float) for item in number_list]), f'number_list must contain all floats'\n",
        "\n",
        "  result = 1.\n",
        "  for number in number_list:  #fancier version of for i in range(n):\n",
        "    result *= number\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntRJT4ePKX2q",
        "outputId": "1bcdf27c-6ae7-4014-f297-24bcb1e34c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#just to test\n",
        "\n",
        "z = [4.,2.,3.]\n",
        "x = float_mult(z)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB_F7Th2KhAk"
      },
      "source": [
        "#glue it all together here.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah9aITm_Lqwc",
        "outputId": "2c5eb048-c316-4996-f484-3aa5a9e3b181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "p_eap_ei  #0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIvi6bhjLBb_"
      },
      "source": [
        "We knew it was going to be zero, right? We saw it in `p_ei_eap_list`. It contained several zeroes.\n",
        "\n",
        "I want to get back to this \"zero thing\" in the next chapter. I don't like it and neither did Laplace. We will see what he did about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHWNdzjyMJYH"
      },
      "source": [
        "##Where we sit\n",
        "\n",
        "We have computed that, according to Naive Bayes, there is zero probability that EAP wrote the sentence. We now need to do the same for MWS and HPL.\n",
        "\n",
        "BTW: let's see who actually did write the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GALuQKOMbzm",
        "outputId": "b6b4c85e-0a00-4223-b05a-0df273050d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "training_authors[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HPL'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USLB49gNM7QU"
      },
      "source": [
        "Our hope is that between the 2 remaining probabilites, HPL will win. Ideal is if he gets 1.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXlAJ99GxN1"
      },
      "source": [
        "##Let's stop here for now\n",
        "\n",
        "We will pick up where we left off in Chapter 6. Kind of a cliff hanger :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L6az8mhte3o"
      },
      "source": [
        "#BTW: why is Bayes Naive?\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif' height=100>\n",
        "\n",
        "Well, Thomas Bayes (circa 1750), himself, was not naive. Or maybe he was, don't know. What is meant by naive is an assumption we make to allow us to use the simplified formula we used above. Look at our sentence again:\n",
        "<pre>\n",
        "['laugh', 'oliver', 'grandfather', 'wo', 'ride', 'motor']\n",
        "</pre>\n",
        "which turns into:\n",
        "<pre>\n",
        "P(EAP|'laugh', 'oliver', 'grandfather', 'wo', 'ride', 'motor')\n",
        "</pre>\n",
        "The non-naive way of looking at it is  where we view all terms as dependent on each other, e.g. we can't view `laugh` on its own but only in conjunction with (conditionally on) `oliver`, `grandfather`, etc. The probability-chain rule formula captures this idea. And sorry for 2nd time, I am changing notation once again: O becomes C, E becomes x:\n",
        "<p>\n",
        "<img src='https://www.dropbox.com/s/v2ppatadfghvrb4/chain2.png?raw=1'>\n",
        "<p>\n",
        "All of these probabilities are known to us so we could calculate an answer.  We would need a table that contains all possible combinations of words with their counts, i.e., the powerset. We have 24K unique words. The size of the powerset is `2**N`. So we are looking at a new word_bag table with 2**24000 items. Just as a heads up, here is 2**150:\n",
        "<pre>\n",
        "2**150\t=\t1, 427, 247, 692, 705, 959, 881, 058, 285, 969, 449, 495, 136, 382, 746, 624\n",
        "</pre>\n",
        "Gulp. That's not going to happen unless you bring in a separate database to store all that info.\n",
        "\n",
        "One alternative is to forget the database and just do on-demand counting for each new set of words. But this could be slow if we have to search the entire set of sentences multiple times to get the powerset numbers.\n",
        "\n",
        "To get around the space and computational costs of the formula above (the full chain rule), Smith said let's pretend that all terms are independent of each other. Others said, don't be naive, Smith, you can't assume that terms are totally independent. But Smith persevered. And she was right, the naive assumption does work well in many cases. So Naive Bayes is normal Bayes but with the strong (naive) assumption that all items are independent.\n",
        "<p>\n",
        "Who was Smith? I don't have a definitive answer. But I like her thinking. Try simple first.\n",
        "</div>"
      ]
    }
  ]
}